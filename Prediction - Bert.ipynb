{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cee440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f87066a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = '/GPU:0' if len(tf.config.list_physical_devices('GPU')) > 0 else '/CPU:0'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db30718",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda7682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (52836, 2)\n"
     ]
    }
   ],
   "source": [
    "# load prediction data\n",
    "directory = 'data/'\n",
    "dataset_path = os.path.join(directory, 'prediction_dataset.csv')\n",
    "dataset_df = pd.read_csv(dataset_path)\n",
    "print(\"Shape:\", dataset_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe13271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    atur kepala badan duduk keluarga rencana nasio...\n",
       "1    atur menteri uang nomor pmk tatacara hitung ba...\n",
       "2    atur menteri uang nomor pmk ubah ata atur ment...\n",
       "3    atur menteri uang nomor pmk kembali bea masuk ...\n",
       "4    atur menteri uang nomor pmk alokasi kurang bay...\n",
       "Name: judul_dokumen, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the text data to be predicted\n",
    "text_df = dataset_df['judul_dokumen']\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f277c",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bab5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer instance\n",
    "bert_model = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n",
    "bert_layer = hub.KerasLayer(bert_model, trainable=False)\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74559945",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4eb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0869f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_input(df):\n",
    "    input_word_ids, input_mask, input_type_ids = [], [], []\n",
    "    for sequence in df:\n",
    "        sequence_tokens = tokenizer.tokenize(sequence)\n",
    "        sequence_tokens = [\"[CLS]\"] + sequence_tokens + [\"[SEP]\"]\n",
    "        \n",
    "        if len(sequence_tokens) > max_seq_length:\n",
    "            sequence_tokens = sequence_tokens[:max_seq_length]\n",
    "\n",
    "        sequence_ids = get_ids(sequence_tokens, tokenizer, max_seq_length)\n",
    "        input_word_ids.append(sequence_ids)\n",
    "\n",
    "        sequence_mask = get_masks(sequence_tokens, max_seq_length)\n",
    "        input_mask.append(sequence_mask)\n",
    "\n",
    "        sequence_segments = get_segments(sequence_tokens, max_seq_length)\n",
    "        input_type_ids.append(sequence_segments)\n",
    "\n",
    "    transformed_seq = dict(\n",
    "        input_word_ids= tf.convert_to_tensor(np.asarray(input_word_ids).astype('int32'), dtype=tf.int32),\n",
    "        input_mask= tf.convert_to_tensor(np.asarray(input_mask).astype('int32'), dtype=tf.int32),\n",
    "        input_type_ids= tf.convert_to_tensor(np.asarray(input_type_ids).astype('int32'), dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    return transformed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab69a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "transformed_seq = build_bert_input(text_df)\n",
    "ds = tf.data.Dataset.from_tensor_slices(transformed_seq)\n",
    "ds = ds.shuffle(text_df.shape[0] // 4).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a772b6",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296ac3d",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c237f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_embedding (KerasLayer)     {'default': (None, 7 109482241   input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           bert_embedding[0][14]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 54)           41526       global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 109,523,767\n",
      "Trainable params: 109,523,766\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load keras saved model\n",
    "saved_model_path = 'bert_kaidah_model.h5'\n",
    "model = tf.keras.models.load_model(saved_model_path, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6ba4e",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b81479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303/3303 [==============================] - 475s 143ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict from text\n",
    "predictions = model.predict(ds, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57744b75",
   "metadata": {},
   "source": [
    "# Prediction Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762590bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_path = os.path.join(directory, 'label.csv')\n",
    "class_df = pd.read_csv(class_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf02062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: putus menteri ppn kepala bappena nomor kep m ppn hk angkat anggota forum masyarakat statistik masa kerja tahun\n",
      "Predicted label: Jabatan Fungsional ( 17 )\n"
     ]
    }
   ],
   "source": [
    "seed = 44388    \n",
    "index = predictions[seed].argmax()\n",
    "print(\"Title:\", dataset_df['judul_dokumen'][seed])\n",
    "print(\"Predicted label:\", class_df['class'][index], '(', index+1, ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2523c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>Lain-lain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>Lain-lain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>Lain-lain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>Lain-lain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>Lain-lain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  subject_id    subject\n",
       "0   1          54  Lain-lain\n",
       "1   2          54  Lain-lain\n",
       "2   3          54  Lain-lain\n",
       "3   4          54  Lain-lain\n",
       "4   5          54  Lain-lain"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_df = pd.DataFrame()\n",
    "saved_df['id'] = dataset_df.index + 1\n",
    "saved_df['subject_id'] = saved_df['id'].apply(lambda x: predictions[x-1].argmax() + 1)\n",
    "saved_df['subject'] = saved_df['subject_id'].apply(lambda x: class_df['class'][x-1])\n",
    "saved_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e00ab8",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc1d8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to directory\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "saved_data_path = os.path.join(directory, 'predicted_jdihn.csv')\n",
    "saved_df.to_csv(saved_data_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
